------------------------------------------------------------------------------------------------------------------------Batch processing with Hadoop HDFS--------------------------------------------------------------------------------------------------------------------------------
1-Download the docker image uploaded to dockerhub:

docker pull liliasfaxi/hadoop-cluster:latest    
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
2-1-Create a network linking the three containers:

docker network create hadoop    **Picture2-1
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
2-2-Create and launch the three containers **Picture2-2

#Launch the Master Node
docker run -iid --net=hadoop -p 9870:9870 -p 8088:8088 -p 7077:7077 -p 16010:16010 --name hadoop-master --hostname hadoop-master liliasfaxi/hadoop-cluster:latest    

#Launch Worker 1
docker run -iid -p 8040:8042 --net=hadoop --name hadoop-worker1 --hostname hadoop-worker1 liliasfaxi/hadoop-cluster:latest

#Launch Worker 2
docker run -iid -p 8041:8042 --net=hadoop --name hadoop-worker2 --hostname hadoop-worker2 liliasfaxi/hadoop-cluster:latest
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
3-2-Check that all three containers are running properly by launching the command: 
  docker ps    **Picture2-3